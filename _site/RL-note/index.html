<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.9.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js" xmlns:wb="http://open.weibo.com/wb">
  <head>
    

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Reinforcement learning notes   Dongda’s homepage</title>
<meta name="description" content="Reinforcement learning notes">
<meta name="keywords" content="Reinforcement learning notes">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_HK">
<meta property="og:site_name" content="Dongda's homepage">
<meta property="og:title" content="Reinforcement learning notes">
<meta property="og:url" content="https://dongdongbh.tech/RL-note/">


  <meta property="og:description" content="Reinforcement learning notes">





  <meta name="twitter:site" content="@dongdongbh">
  <meta name="twitter:title" content="Reinforcement learning notes">
  <meta name="twitter:description" content="Reinforcement learning notes">
  <meta name="twitter:url" content="https://dongdongbh.tech/RL-note/">

  
    <meta name="twitter:card" content="summary">
    
  

  



  <meta property="article:published_time" content="2018-12-01T00:00:00+08:00">





  

  


<link rel="canonical" href="https://dongdongbh.tech/RL-note/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Dongda Li",
      "url": "https://dongdongbh.tech",
      "sameAs": null
    }
  </script>



  <meta name="google-site-verification" content="google666c99d8bc51b024.html" />


  <meta name="baidu-site-verification" content="CRSaXcVqvi" />





<!-- end _includes/seo.html -->


<link href="https://dongdongbh.tech/feed.xml" type="application/atom+xml" rel="alternate" title="Dongda's homepage Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://dongdongbh.tech/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->
<meta property="wb:webmaster" content="ec24ee5aab20f47f" />
<script src="https://tjs.sjs.sinajs.cn/open/api/js/wb.js" type="text/javascript" charset="utf-8"></script>
<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->
  </head>

  <body class="layout--single wide">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="https://dongdongbh.tech/">Dongda's homepage</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="https://dongdongbh.tech/year-archive/" >Posts</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="https://dongdongbh.github.io/note/" >Notes</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="https://dongdongbh.tech/cv/" >CV</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="https://mmistakes.github.io/minimal-mistakes/about/" >About</a>
            </li>
          
        </ul>
        
        <button class="search__toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="https://dongdongbh.tech/assets/images/bio-photo.jpg" alt="Dongda Li" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Dongda Li</h3>
    
    
      <p class="author__bio" itemprop="description">
        Hello world!
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Hongkong, China</span>
        </li>
      

      

      
        <li>
          <a href="mailto:dongdongbhbh@gmail.com">
            <meta itemprop="email" content="dongdongbhbh@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      
        <li>
          <a href="https://github.com/dongdongbh" itemprop="sameAs">
            <i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      
        <li>
          <a href="https://www.weibo.com/513485234" itemprop="sameAs">
            <i class="fab fa-fw fa-weibo" aria-hidden="true"></i> Weibo
          </a>
        </li>
      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Reinforcement learning notes">
    <meta itemprop="description" content="Table of contents  Basic  Cross-entropy method  Tabular Learning  DQN  Policy Gradients  DRL in NLP  NN functions">
    <meta itemprop="datePublished" content="December 01, 2018">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Reinforcement learning notes
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  8 minute read
</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="table-of-contents">Table of contents</h2>
<ul>
  <li><a href="#1.Basic">Basic</a></li>
  <li><a href="#cross-entropy-method">Cross-entropy method</a></li>
  <li><a href="#tabular-learning">Tabular Learning</a></li>
  <li><a href="#deep-q-learning">DQN</a></li>
  <li><a href="#policy-gradients">Policy Gradients</a></li>
  <li><a href="#Deep-Reinforcement-Learning (Deep RL) in-Natural-Language-Processing (NLP)">DRL in NLP</a></li>
  <li><a href="#nn-functions">NN functions</a></li>
</ul>

<h2 id="basic">basic</h2>

<h5 id="markov-decision-process-mdp">Markov Decision Process (MDP)</h5>

<p><img src="../assets/images/rl/reinforcement-learning-fig1-700.jpg" width="70%" /></p>

<p>environment, state, observation, reward, action, agent</p>

<h5 id="policy">Policy</h5>

<script type="math/tex; mode=display">\pi: S \times A \to [0,1]\\ \label{l1}
\pi(a|s) = P(a_t=a|s_t=s)</script>

<h5 id="state-value-function">State-value function</h5>

<script type="math/tex; mode=display">R = \Sigma_{t = 0}^{\infty}\gamma^tr_t\\ \label{l2}
V_\pi(s) = E[R] = E[\Sigma_{t = 0}^{\infty}\gamma^tr_t|s_0 = s]\\</script>

<p>where $r_t$ is the reward at step $t$,  $\gamma\in[0,1]$  is the discount-rate.</p>

<h5 id="value-function">Value function</h5>

<script type="math/tex; mode=display">V^\pi(s)  =E[R|s,\pi]\\	\label{l3}
V^*(s) = \max_\pi V^\pi(s)</script>

<h5 id="action-value-function">Action value function</h5>

<script type="math/tex; mode=display">Q^\pi(s,a) = E[R|s,a,\pi]\\ \label{l4}
Q^*(s) = \max_a Q^\pi(s,a)</script>

<h3 id="method-classification">method classification</h3>

<ul>
  <li>model-based: previous observation <strong>predict</strong> following rewards and observations</li>
  <li>model-free: train it by intuition</li>
  <li>police-based: <strong>directly</strong> approximating the policy of the agent</li>
  <li>value-based: the agent calculates the <strong>value of every possible action</strong></li>
  <li>off police: the ability of the method to learn on old <strong>historical data</strong> (obtained</li>
  <li>on police: requires <strong>fresh data</strong> obtained from the environment</li>
</ul>

<h3 id="police-based-method">Police-based method</h3>

<p><strong>just like a classification problem</strong></p>

<ul>
  <li>NN input: observation</li>
  <li>NN output: distribution of actions</li>
  <li>agent: random choose action base on distribution of actions(police)</li>
</ul>

<h2 id="cross-entropy-method">cross-entropy method</h2>

<h4 id="steps">steps:</h4>

<ol>
  <li>Play N number of episodes using our current model and environment.</li>
  <li>Calculate the total reward for every episode and decide on a reward boundary. Usually, we use some percentile of all rewards, such as 50th or 70th.</li>
  <li>Throw away all episodes with a reward below the boundary.</li>
  <li>Train on the remaining “elite” episodes using observations as the input and issued actions as the desired output.</li>
  <li>Repeat from step 1 until we become satisfied with the result.</li>
</ol>

<p>use <strong>cross-entropy loss</strong> function as loss function</p>

<p><strong>drawback:</strong> Cross-entropy methods have difficult to understand which step or which state is good and which is not good, it just know overall this episode is better or not</p>

<h2 id="tabular-learning">tabular learning</h2>

<h3 id="why-using-q-but-not-v">Why using Q but not V?</h3>

<p>​	if I know the value of current state, I know the state is good or not, but I don’t know how to choose next action, even I know the V of all next state, I <strong>can not directly</strong> know which action i need to do, so we decide action base on Q.</p>

<p>​	if I know Q of all available action, we just choose the action which has max Q, then this action surely has max V according the definition of V(the relationship of Q and V).</p>

<h3 id="the-value-iteration-in-the-env-with-a-loop">The value iteration in the Env with a loop</h3>

<p>If there is no $\gamma (\gamma = 1)$ and the environment has a loop, the value of state will be infinite.</p>

<h3 id="problems--in-q-learning">problems  in Q-learning</h3>

<ul>
  <li>state is not discrete</li>
  <li>state space is is very large</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>don’t know probability of action and reward matrix (P(s’,r</td>
          <td>s,a)).</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h3 id="value-iteration">Value iteration</h3>

<h4 id="reward-table">Reward table</h4>

<ul>
  <li>index: “source state” + “action” + “target state”</li>
  <li>value: reward</li>
</ul>

<h4 id="transition-table">Transition table</h4>

<ul>
  <li>index: “state” + “action”</li>
  <li>value:  index: state  value: counts</li>
</ul>

<h4 id="value-table">Value table</h4>

<ul>
  <li>index: state</li>
  <li>value:  value of state</li>
</ul>

<h4 id="steps-1">Steps</h4>

<ol>
  <li>
    <p>random action to build reward and transitions table</p>
  </li>
  <li>
    <p>perform a value iteration loop over all state</p>
  </li>
  <li>
    <p>play several full episodes to choose the best action using the updated value table, at the same time, update reward and transitions table using new data.</p>
  </li>
</ol>

<p>**Problems of separating training and testing **: When using the previous steps, you actually separate training and testing, it may has another problem, since the task may be difficult,using random action is hard to reach the final state, so you may lack some states which are near the final step. So, maybe you should conduct training and testing at the same time, and add some exploit into testing.</p>

<h3 id="q-learning">Q-learning</h3>

<p>Different to value iteration,Q-learn change the value table to Q value table:</p>

<h4 id="q-value-table">Q value table</h4>

<ul>
  <li>index:  “state” + “action”</li>
  <li>value:  action value(Q)</li>
</ul>

<p>Here :
<script type="math/tex">V(s) =  \mathop{\arg\max}_{a}Q(a,s)</script></p>

<h2 id="deep-q-learning">deep q-learning</h2>

<h4 id="dqn">DQN:</h4>

<p>input: state</p>

<p>output: all action(n actions) value of input state</p>

<p>classification: off policy, value based and model free</p>

<h4 id="problems">problems:</h4>

<ul>
  <li>how to balance explore&amp;exploit</li>
  <li>data is not independent and identically distributed(i.i.d), which is required for neural network training.</li>
  <li>may partially observable MDPs (<strong>POMDP</strong>)</li>
</ul>

<h4 id="basic-tricks-in-deepmind-2015-paper">Basic tricks in Deepmind 2015 paper:</h4>

<ul>
  <li>$\epsilon$-greedy to deal with  explore&amp;exploit</li>
  <li>replay buffer and target network to deal with i.i.d,
    <ul>
      <li>replay buffer make it more random, it random select experience in a replay buffer</li>
      <li>target network isolated the influence of nearby Q during training</li>
    </ul>
  </li>
  <li>several observations as a state to deal with POMDP</li>
</ul>

<h4 id="double-dqn">Double DQN</h4>

<p><strong>Idea:</strong> Choosing <strong>actions</strong> for the next state using the <strong>trained network</strong> but taking <strong>values of Q from the target net</strong>.</p>

<h4 id="noisy-networks">Noisy Networks</h4>

<p><strong>Idea:</strong> Add <strong>a noise to the weights of fully-connected layers</strong> of the network and adjust the parameters of this noise during training using back propagation. (to replace $\epsilon$-greedy and improve performance)</p>

<h4 id="prioritized-replay-buffer">Prioritized replay buffer</h4>

<p><strong>Idea:</strong> This method tries to improve the efficiency of samples in the replay buffer by <strong>prioritizing those samples according to the training loss</strong>.</p>

<p><strong>Trick:</strong> using loss weight to compensated the distribution bias introduced by priorities.</p>

<h4 id="dueling-dqn">Dueling DQN</h4>

<p><strong>Idea:</strong> The Q-values Q(s, a) our network is trying to approximate can be divided into quantities: the value of the state V(s) and the advantage of actions in this state A(s, a).</p>

<p><strong>Trick:</strong> the mean value of the advantage of any state to be zero.</p>

<h4 id="categorical-dqn">Categorical DQN</h4>

<p><strong>Idea:</strong> Train the probability distribution of action Q-value rather than the action Q-value</p>

<p><strong>Tricks:</strong></p>

<ul>
  <li>
    <p>using generic parametric distribution that is basically a fixed amount of values placed regularly on a values range. every fixed amount of values range calls atom.</p>
  </li>
  <li>
    <p>use loss Kullback- Leibler (KL)-divergence</p>
  </li>
</ul>

<h2 id="policy-gradients">policy gradients</h2>

<h3 id="reinforce">REINFORCE</h3>

<h4 id="idea">idea</h4>

<p>Policy Gradient
<script type="math/tex">\Delta J \approx E[Q(s,a)\Delta\log\pi(a|s)] \label{l5}</script>
loss formula
<script type="math/tex">loss = -Q(s,a)\log\pi(a|s) \label{l6}</script>
Increase the probability of actions that have given us good total reward and decrease the probability of actions with bad final outcomes.
<script type="math/tex">{split}
\pi(a|s)>0\\
-\log\pi(a|s) > 0 \label{l7}</script> {split}</p>
<h4 id="problems-1"><strong>problems:</strong></h4>

<ul>
  <li>one training need full episodes since require Q from finished episode</li>
  <li>
    <p>High gradients variance, long steps episode have larger Q than short one</p>
  </li>
  <li>converge to some locally-optimal policy since lack of exploration</li>
  <li>not i.i.d. Correlation between samples</li>
</ul>

<h4 id="basic-tricks">basic tricks</h4>

<ul>
  <li>learning Q(Actor-Critic)</li>
  <li>subtracting a value called baseline from the Q to avoid high gradients variance</li>
  <li>in order to prevent our agent from being stuck in the local minimum, subtracting the entropy from the loss function, punishing the agent for being too certain about the action to take.</li>
  <li>parallel environments to reduce <strong>correlation</strong>, steps from different environments.</li>
</ul>

<h3 id="actor--critic">Actor- Critic</h3>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
Q(s,a) &= \Sigma_{i=0}^{N-1}\gamma^ir_i+\gamma^NV(s_N)\\ 
Loss_{value} &= MSE(V(s),Q(s,a))\\ \label{Q_update}
\end{aligned}
\end{equation} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{aligned}
Q(s,a) &= A(s,a)+V(s)\\ 
Loss_{policy} &= -A(s,a)\log\pi(a|s)\\  \label{pg_update}
\end{aligned}
\end{equation} %]]></script>

<p>Using equation \ref{Q_update} to train V(s) (Critic) and equation \ref{pg_update} to train policy. We call A(s,a) as advantage,  so it is advantage Actor- Critic (<strong>A2C)</strong>.</p>

<p><strong>Idea</strong>: The scale of our gradient will be just advantage A(s, a), we use another neural network, which will approximate V(s) for every observation.</p>

<h4 id="implementation">Implementation</h4>

<p>In practice, policy and value networks partially overlap, mostly due to the efficiency and convergence considerations. In this case, policy and value are implemented as different heads of the network, taking the output from the common body and transforming it into the probability distribution and a single number representing the value of the state. This helps both networks to share low-level features, but combine them in a different way.</p>

<h4 id="tricks">Tricks</h4>

<ul>
  <li>
    <p>add entropy bonus to loss function
<script type="math/tex">H_{entropy} = -\Sigma (\pi \log\pi)   \\
Loss_{entropy} = \beta*\Sigma_i (\pi_\theta(s_i)*\log\pi_\theta(s_i)) \label{l10}</script></p>

    <blockquote>
      <p>the loss function of entropy has a minimum when probability distribution is uniform, so by adding it to the loss function, we’re pushing our agent away from being too certain about its actions.</p>
    </blockquote>
  </li>
  <li>
    <p>using several environments to improve stability</p>
  </li>
  <li>
    <p>gradient clipping to prevents our gradients at optimization stage from becoming too large and pushing our policy too far.</p>
  </li>
</ul>

<h4 id="total-loss-function">Total Loss function</h4>

<p>Finally, our loss is the sum of PG, value and entropy loss
<script type="math/tex">Loss =Loss_{policy}+Loss_{value}+Loss_{entropy}</script></p>

<h4 id="asynchronous-advantage-actor-critica3c">Asynchronous Advantage Actor-Critic(A3C)</h4>

<blockquote>
  <p>Just using parallel envs to speed up training, there will be some code level tricks to speed up by fully utilizing multiple GPUs and CPUs. For more details, ref some open source implementations on Github.</p>
</blockquote>

<h2 id="deep-reinforcement-learning-deep-rl-in-natural-language-processing-nlp">Deep Reinforcement Learning (Deep RL) in Natural Language Processing (NLP)</h2>

<h3 id="basic-concepts-in-nlp">Basic concepts in NLP</h3>

<ul>
  <li>Recurrent Neural Networks (RNNs)</li>
  <li>word embeddings</li>
  <li>the <strong>seq2seq</strong> model</li>
  <li><a href="http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention">Recurrent models of visual attention</a> (original paper NIPS 2014)</li>
</ul>

<p>Ref. <a href="http://cs224d.stanford.edu">CS224d</a> for more about NLP.</p>

<h4 id="rnn">RNN</h4>

<p>The idea of an RNN is a network with fixed input and output, which is being applied to the sequence of objects and can pass information along this sequence. This information is called hidden state and is normally just a vector of numbers of some size.</p>

<p><strong>Unfold RNN (unfold by time)</strong></p>

<p><img src="../assets/images/rl/Recurrent_neural_network_unfold.svg.png" width="70%" /></p>

<p>RNN produce different output for the same input in different contexts, RNNs can be seen as a standard building block of the systems that need to process variable-length input.</p>

<h5 id="lstm">LSTM</h5>

<p><img src="../assets/images/rl/800px-Long_Short-Term_Memory.svg.png" width="70%" /></p>

<h4 id="word-embeddingword2vec">Word embedding(word2vec)</h4>

<p>Word and phrase embeddings, is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where <strong>words or phrases from the vocabulary are mapped to vectors of real numbers</strong>. Conceptually it involves a mathematical embedding from a space with <strong>one dimension per word to a continuous vector space with a much lower dimension</strong>.</p>

<p><strong>Methods</strong> to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models,  explainable knowledge base method, and explicit representation in terms of the context in which words appear.</p>

<p>Word embedding is good for NLP tasks  such as syntactic parsing and sentiment analysis. Ref <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> for details.</p>

<p>You can use some pretrained dataset or get it by training your own dataset.</p>

<h4 id="encoder-decoderseq2seq">Encoder-Decoder(seq2seq)</h4>

<p><img src="../assets/images/rl/seq2seq.jpg" width="70%" /></p>

<p>use an RNN to process an input sequence and encode this sequence into some fixed-length representation. This RNN is called an encoder. Then you feed the encoded vector into another RNN, called a decoder, which has to produce the resulting sequence. It is widely used in machine translation.</p>

<ul>
  <li>
    <p><strong>teacher-forcing mode</strong>: decoder input is the target reference</p>
  </li>
  <li>
    <p><strong>curriculum learning mode</strong>: decoder input is the last out put of previous decoder</p>

    <table>
      <thead>
        <tr>
          <th style="text-align: center"><img src="../assets/images/rl/curriculum learning.jpg" width="70%" /></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center">curriculum learning mode</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p><strong>attention mechanism</strong></p>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="../assets/images/rl/nmt-model-fast.gif" width="70%" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">seq2seq (picture from Google)</td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="../assets/images/rl/attention.jpg" width="70%" /></td>
    </tr>
    <tr>
      <td style="text-align: center">attention mechanism (this picture from <a href="https://zhuanlan.zhihu.com/p/40920384">zhihu</a>)</td>
    </tr>
  </tbody>
</table>

<h3 id="rl-in-seq2seq">RL in seq2seq</h3>

<ul>
  <li>sampling from probability distribution, instead of learning some average result</li>
  <li>score is not differentiable, we still can use PG to update, use score as scale</li>
  <li>introducing stochasticity into the process of decoding when dataset is limited</li>
  <li>use argmax score as baseline of Q</li>
</ul>

<h2 id="ddpg">DDPG</h2>

<p>TBC.</p>

<h2 id="model-based-rl">Model-based RL</h2>

<p>TBC.</p>

<h2 id="nn-functions">nn functions</h2>

<h4 id="sigmoid">sigmoid</h4>

<p><em><u>It transfer a value input to (0,1)</u></em></p>

<script type="math/tex; mode=display">f(x)=\frac{L}{1+e^{-x}} = \frac{e^{x}}{e{x}+1}</script>

<h4 id="softmax"><strong>softmax</strong></h4>

<p>In short, <em><u>It transfer K-dimensional vector input to (0,1)</u></em></p>

<p>In mathematics, the softmax function, or normalized exponential function, is a generalization of the logistic function that “squashes” a K-dimensional vector <strong>z</strong>  of arbitrary real values to a K-dimensional vector  \sigma(<strong>z</strong>) of real values, where each entry is in the range (0, 1), and all the entries add up to 1.</p>

<h4 id="tanh">tanh</h4>

<p><em><u>It transfer a value input to (-1,1)</u></em></p>

<script type="math/tex; mode=display">f(x)=tanh(x)= \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}</script>

<h4 id="relu"><strong>relu</strong></h4>

<script type="math/tex; mode=display">f(x)=max(0,x)</script>

<h2 id="reference">Reference</h2>

<ul>
  <li>
    <p>Maxim Lapan, Deep Reinforcement Learning Hands-On  2018</p>
  </li>
  <li>Mnih V, Kavukcuoglu K, Silver D, et al. Human-level control through deep reinforcement learning[J]. Nature, 2015, 518(7540): 529.</li>
  <li>Mnih V, Heess N, Graves A. Recurrent models of visual attention[C]//Advances in neural information processing systems. 2014: 2204-2212.</li>
  <li>Paszke, Adam and Gross, etc.  Automatic differentiation in PyTorch, 2017</li>
</ul>


        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="https://dongdongbh.tech/tags/#note" class="page__taxonomy-item" rel="tag">note</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2018-12-01T00:00:00+08:00">December 01, 2018</time></p>
        
      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?via=dongdongbh&text=Reinforcement+learning+notes%20https%3A%2F%2Fdongdongbh.tech%2FRL-note%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdongdongbh.tech%2FRL-note%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=https%3A%2F%2Fdongdongbh.tech%2FRL-note%2F" class="btn btn--google-plus" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Google Plus"><i class="fab fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

<!--   <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fdongdongbh.tech%2FRL-note%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a> -->
  
  <wb:share-button appkey="2832443974" title=Reinforcement+learning+notes addition="number" type="button"></wb:share-button>

</section>


      
  <nav class="pagination">
    
      <a href="https://dongdongbh.tech/blog/vps/" class="pagination--pager" title="Using Google cloud to build a Virtual Private Server (VPS)
">Previous</a>
    
    
      <a href="https://dongdongbh.tech/CNN-dimension/" class="pagination--pager" title="Convolutional Neural Networks dimension
">Next</a>
    
  </nav>

    </div>

    
      <div class="page__comments">
  
  
      <h4 class="page__comments-title">Comments</h4>
      <section id="g-comments" class="g-comments">Loading Google+ Comments ...</section>
      <script>
        function initComment() {
          gapi.comments.render("g-comments", {
            href: "https://dongdongbh.tech/RL-note/",
            width: "624",
            first_party_property: "BLOGGER",
            view_type: "FILTERED_POSTMOD"
          })
        }
      </script>
      <script async type="text/javascript" src="https://apis.google.com/js/plusone.js" onload="initComment()" />
      <noscript>Please enable JavaScript to view the <a href="https://plus.google.com/">comments powered by Google+.</a></noscript>-->
    
</div>
    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src=
          
            "https://dongdongbh.tech/assets/images/markup-syntax-highlighting-teaser.jpg"
          
          alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://dongdongbh.tech/resource/create-website/" rel="permalink">Create your website on cloud
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  2 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Create your website on Virtual Private Server(VPS)

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://dongdongbh.tech/CNN-dimension/" rel="permalink">Convolutional Neural Networks dimension
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">convolution operation: share the convolution core

</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://dongdongbh.tech/blog/vps/" rel="permalink">Using Google cloud to build a Virtual Private Server (VPS)
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">set up a VPS on Google cloud

  buy a VM on Google cloud (it has one year free trial now)
 choose Debian Linux and the place near to get fast access (eg, asi...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src=
          
            "https://dongdongbh.tech/assets/images/markup-syntax-highlighting-teaser.jpg"
          
          alt="">
      </div>
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://dongdongbh.tech/markup/file-transport/" rel="permalink">Transfer files over a LAN between two Linux computers
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">netcat + tar (fast but not secure)
To send a directory, cd to inside the directory whose contents you want to send on the computer doing the sending and do:

</p>
  </article>
</div>
        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap">
  <input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  <div id="results" class="results"></div>
</div>
      </div>
    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
    
      <li><a href="https://twitter.com/dongdongbh"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
    
    
      <li><a href="https://www.facebook.com/dongdongbh"><i class="fab fa-fw fa-facebook-square" aria-hidden="true"></i> Facebook</a></li>
    
    
      <li><a href="https://github.com/dongdongbh"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    
    <li><a href="https://dongdongbh.tech/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2019 Dongda Li. Powered by <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>. Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a></div>
      </footer>
    </div>

    
  <script src="https://dongdongbh.tech/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.6/js/all.js"></script>



  
  
  <script src="https://dongdongbh.tech/assets/js/lunr/lunr.min.js"></script>
  <script src="https://dongdongbh.tech/assets/js/lunr/lunr-store.js"></script>
  <script src="https://dongdongbh.tech/assets/js/lunr/lunr-en.js"></script>





  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114465156-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-114465156-1', { 'anonymize_ip': false});
</script>






    <script async type="text/javascript" src="//apis.google.com/js/plusone.js?callback=gpcb"></script>
<noscript>Please enable JavaScript to view the <a href="https://plus.google.com/">comments powered by Google+.</a></noscript>
  



  </body>
</html>